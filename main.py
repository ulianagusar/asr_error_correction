import streamlit as st
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline
import re
import kenlm
import tempfile
import os
from io import BytesIO
import soundfile as sf
import numpy as np
from audio_recorder_streamlit import audio_recorder
from prep_data import add_sim_panphon  


st.set_page_config(
    page_title="–ú–µ–¥–∏—á–Ω–∞ ASR –∫–æ—Ä–µ–∫—Ü—ñ—è",
    page_icon="ü©∫",
    layout="wide"
)


@st.cache_resource
def load_models():

    device = "cuda" if torch.cuda.is_available() else "cpu"
    
  
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model_path = "/Users/ulanagusar/Desktop/4_–∫—É—Ä—Å/diplom/Uni_Syn_Med/model_no_nlt_8ep_final_promt/medical_asr_correction_model_best"
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    model.to(device)
    model.eval()
    

    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model="openai/whisper-tiny.en",
        chunk_length_s=30
    )
    

    kenlm_model = kenlm.Model("3gram.bin")
    
    return tokenizer, model, asr_pipe, kenlm_model, device

def clean_text(text):

    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def predict_single(text, tokenizer, model, device, max_length=128):
    
    PROMPT = "correct the ASR output, use phonetically similar words in brackets: "

    inputs = tokenizer(
        PROMPT + text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length
    ).to(device)
    
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_beams=4,
            early_stopping=True
        )
    
    prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return prediction

def process_audio_file(audio_file, asr_pipe):

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        tmp_file.write(audio_file.read())
        tmp_path = tmp_file.name
    
    try:

        result = asr_pipe(tmp_path)
        return result["text"]
    finally:

        os.unlink(tmp_path)

def process_recorded_audio(audio_bytes, asr_pipe):

    if audio_bytes:

        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_bytes)
            tmp_path = tmp_file.name
        
        try:

            result = asr_pipe(tmp_path)
            return result["text"]
        finally:

            os.unlink(tmp_path)
    return None


with st.spinner("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π..."):
    tokenizer, model, asr_pipe, kenlm_model, device = load_models()


st.title("–°–∏—Å—Ç–µ–º–∞ –∫–æ—Ä–µ–∫—Ü—ñ—ó —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è")



with st.sidebar:
    st.header("–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è")
    st.info("""
    **–Ø–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
    1. –û–±–µ—Ä—ñ—Ç—å —Å–ø–æ—Å—ñ–± –≤–≤–µ–¥–µ–Ω–Ω—è –∞—É–¥—ñ–æ
    2. –ó–∞–ø–∏—à—ñ—Ç—å –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –∞—É–¥—ñ–æ
    3. –û—Ç—Ä–∏–º–∞–π—Ç–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏–π —Ç–µ–∫—Å—Ç
    4. –ü–µ—Ä–µ–≥–ª—è–¥—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–æ—Ä–µ–∫—Ü—ñ—ó
    """)
    
    st.header(" –ü–∞—Ä–∞–º–µ—Ç—Ä–∏")
    show_intermediate = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç–∏ –ø—Ä–æ–º—ñ–∂–Ω—ñ –∫—Ä–æ–∫–∏", True)


col1, col2 = st.columns([1, 1])

with col1:
    st.header("–í–≤–µ–¥–µ–Ω–Ω—è –∞—É–¥—ñ–æ")
    

    input_method = st.radio(
        "–û–±–µ—Ä—ñ—Ç—å —Å–ø–æ—Å—ñ–±:",
        ["–ó–∞–ø–∏—Å–∞—Ç–∏ –∞—É–¥—ñ–æ", "–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª"]
    )
    
    recognized_text = None
    
    if input_method == "–ó–∞–ø–∏—Å–∞—Ç–∏ –∞—É–¥—ñ–æ":
        st.subheader("–ó–∞–ø–∏—Å –∞—É–¥—ñ–æ")
        audio_bytes = audio_recorder(
            text="–ù–∞—Ç–∏—Å–Ω—ñ—Ç—å –¥–ª—è –∑–∞–ø–∏—Å—É",
            recording_color="#e74c3c",
            neutral_color="#34495e",
            icon_name="microphone",
            icon_size="2x"
        )
        
        if audio_bytes:
            st.audio(audio_bytes, format="audio/wav")
            
            if st.button("–†–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è", type="primary"):
                with st.spinner("–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è..."):
                    try:
                         recognized_text = process_recorded_audio(audio_bytes, asr_pipe)
                    except ValueError as e:
                          recognized_text = "no_data"
                    
    
    else:  
        st.subheader("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É")
        uploaded_file = st.file_uploader(
            "–û–±–µ—Ä—ñ—Ç—å –∞—É–¥—ñ–æ—Ñ–∞–π–ª",
            type=["wav", "mp3",  "flac"],
            help="–ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏: WAV, MP3, FLAC"
        )
        
        if uploaded_file is not None:
            st.audio(uploaded_file, format="audio/wav")
            
            if st.button("–†–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è", type="primary"):
                with st.spinner("–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è..."):
                    try:
                         recognized_text = process_audio_file(uploaded_file, asr_pipe)
                    except ValueError as e:
                          recognized_text = "no_data"
                         
with col2:
    st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏")
    if recognized_text == "no_data":
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –∞—É–¥—ñ–æ , —Å–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø–∏—Å–∞—Ç–∏ —â–µ —Ä–∞–∑")
    elif recognized_text:
        cleaned_text = clean_text(recognized_text)
        
        st.subheader("–†–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏–π —Ç–µ–∫—Å—Ç:")
        st.text_area("", value=recognized_text, height=100, disabled=True)
        
        if show_intermediate:
            st.subheader("–û—á–∏—â–µ–Ω–∏–π —Ç–µ–∫—Å—Ç:")
            st.text_area("", value=cleaned_text, height=80, disabled=True)
        

        with st.spinner("–û–±—Ä–æ–±–∫–∞ —Ç–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è —Ç–µ–∫—Å—Ç—É..."):
            try:
   
                text_phon_add = add_sim_panphon(cleaned_text, kenlm_model)
                
                if show_intermediate:
                    st.subheader("–¢–µ–∫—Å—Ç –∑ —Ñ–æ–Ω–µ—Ç–∏—á–Ω–∏–º–∏ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏:")
                    st.text_area("", value=text_phon_add, height=100, disabled=True)
                

                corrected_text = predict_single(
                    text_phon_add, tokenizer, model, device, 128
                )
                
  
                st.subheader("–í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç:")
                st.text_area(
                    "", 
                    value=corrected_text, 
                    height=100, 
                    disabled=True,
                    key="final_result"
                )
                
                
            except Exception as e:
                st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ: {str(e)}")
    
    else:
        st.info("üëÜ –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø–∏—à—ñ—Ç—å –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –∞—É–¥—ñ–æ")

